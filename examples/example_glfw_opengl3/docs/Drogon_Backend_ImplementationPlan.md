# Stage 1 Dataset & Simulation Service – Drogon Backend Plan

The goal is to move all persistence and dataset management logic off individual desktops and onto a dedicated Drogon service hosted on the Stage 1 backend (45.85.147.236 / agenticresearch.info). The Drogon API becomes the single integration point for both the Dear ImGui desktop app and future web frontends, while Postgres + QuestDB remain the source of truth for metadata and time-series payloads.

---

## 1. High-Level Architecture

```
Dear ImGui Desktop ─┐
                    ├─> Drogon HTTP/WS API ──> Postgres (metadata/table joins)
Web Frontend (future) ┘                       │
                                               └─> QuestDB (OHLCV, indicators, predictions, trades)
```

Key principles:
1. **Single API surface** – Clients never connect directly to Postgres/QuestDB; they call Drogon endpoints.
2. **Stateless controllers** – Drogon controllers perform input validation, authenticate requests, then delegate to service classes that encapsulate Postgres/QuestDB logic.
3. **Async I/O** – Drogon naturally supports async query execution; leverage its Postgres ORM for metadata and libpqxx/pq API for any raw SQL.
4. **Clear lineage** – Every dataset/run/simulation response should include IDs, slugs, measurement names, and ready-to-use URLs for streaming large CSV payloads out of QuestDB.

---

## 2. Environment & Credentials

| Component  | Host / Port            | Credentials (current)                                  |
|------------|------------------------|--------------------------------------------------------|
| Postgres   | 45.85.147.236:5432     | `stage1_app / TempPass2025` (scram-sha-256)            |
| QuestDB    | ILP 9009 / REST 9000   | HTTP basic auth disabled; internal network only        |
| Drogon SSL | agenticresearch.info   | Use existing TLS cert (likely /etc/letsencrypt/live/…) |

**Configuration strategy**
- Store credentials in `/etc/stage1_backend/.env` loaded by Drogon at startup (use `dotenv` pattern or Drogon config JSON).
- For QuestDB, keep the ILP/REST base URLs configurable (default `http://45.85.147.236:9000` / `45.85.147.236:9009`).
- Expose Postgres via Drogon’s ORM config (`drogon_ctl create model` + `config.json`).

---

## 3. Database Touchpoints

### 3.1 Postgres

Existing schema (post Stage 1.3 + dataset task list):
- `stage1_datasets` (canonical dataset triplets)
- `indicator_datasets` (legacy table still referenced by FKs)
- `walkforward_runs` / `walkforward_folds`
- `simulation_runs` / `simulation_trade_buckets` / `simulation_trades`
- Views/functions: `dataset_catalog`, `dataset_hierarchy`, `fn_list_datasets`, `fn_list_runs`, `fn_list_simulations`, `upsert_stage1_dataset(...)`

**ORM Models**
Use `drogon_ctl create model stage1_datasets indicator_datasets walkforward_runs simulation_runs ...`
- `Stage1Dataset` – maps to `stage1_datasets`
- `IndicatorDataset` – used only for compatibility
- `WalkforwardRun`, `WalkforwardFold`
- `SimulationRun`, `SimulationTradeBucket`, `SimulationTrade`

**Raw SQL helpers**
- Continue using `upsert_stage1_dataset()` for dataset writes.
- Introduce stored procedures for walkforward/simulation inserts if necessary; otherwise wrap the insert logic in a service class with transactions.

### 3.2 QuestDB

Direct HTTP/ILP calls from Drogon:
- **Exports:** use ILP via TCP (`boost::asio`) or reuse the existing C++ exporter (port it into a Drogon service). We already know measurement naming and timestamp coercion rules.
- **Imports:** hit `/exp?query=...&fmt=csv` and stream results to the client (or convert to Arrow + JSON depending on endpoint).

**Nuances we’ve learned:**
- `/exec` returns JSON; use `/exp` for CSV.
- Timestamps may be stored in `timestamp` column; convert to millis manually if needed.
- Large payloads (predictions/trades) should be streamed rather than loaded entirely into memory.

---

## 4. Drogon Project Layout

```
stage1_server/
  config.json
  .env (not committed)
  models/           (generated by drogon_ctl create model)
  controllers/
    DatasetController.h/.cc
    RunController.h/.cc
    SimulationController.h/.cc
    QuestDbProxyController.h/.cc (CSV streaming, websockets)
  services/
    DatasetService.h/.cc       (wrappers around Postgres + QuestDB ops)
    RunPersistenceService.h/.cc
    QuestDbService.h/.cc
  utils/
    EnvConfig.h/.cc            (load .env, secrets)
    CsvStreamResponder.h/.cc   (stream QuestDB responses to clients)
```

`config.json` example snippet:
```json
{
  "listeners": [
    {
      "address": "0.0.0.0",
      "port": 8443,
      "https": true,
      "cert": "/etc/letsencrypt/live/agenticresearch.info/fullchain.pem",
      "key": "/etc/letsencrypt/live/agenticresearch.info/privkey.pem"
    }
  ],
  "db_clients": [
    {
      "rdbms": "postgresql",
      "host": "45.85.147.236",
      "port": 5432,
      "dbname": "stage1_trading",
      "user": "stage1_app",
      "password": "${STAGE1_POSTGRES_PASSWORD}",
      "name": "stage1"
    }
  ]
}
```
(Leverage Drogon environment variable substitution for the password.)

---

## 5. API Surface

### 5.1 Authentication
- Start with token-based auth via request header (e.g., `X-Stage1-Token`). Tokens stored/hashed in Postgres.
- Eventually integrate OAuth/JWT if needed for the frontend.

### 5.2 REST Endpoints

| Method | Path                           | Description                                                     |
|--------|--------------------------------|-----------------------------------------------------------------|
| GET    | `/api/datasets`                | Paginated list from `fn_list_datasets`, includes row counts     |
| POST   | `/api/datasets`                | Create/update dataset metadata + trigger QuestDB exports        |
| GET    | `/api/datasets/{id}`           | Detailed dataset info + linked runs/sims                        |
| POST   | `/api/datasets/{id}/export`    | Accepts OHLCV + indicator payloads (CSV upload or presigned URL)|
| GET    | `/api/runs`                    | List runs (filters: dataset_id, slug, status)                   |
| POST   | `/api/runs`                    | Persist walkforward run + folds + predictions                   |
| GET    | `/api/runs/{id}`               | Run details + fold summary                                      |
| GET    | `/api/runs/{id}/predictions`   | Stream QuestDB predictions CSV                                  |
| GET    | `/api/simulations`             | List simulations (filters: run_id, dataset_id)                  |
| POST   | `/api/simulations`             | Persist simulation metadata + trades                            |
| GET    | `/api/simulations/{id}`        | Simulation summary                                              |
| GET    | `/api/simulations/{id}/trades` | Stream QuestDB trades CSV                                       |

### 5.3 WebSocket (future)
- `/ws/streams/{dataset_slug}` for live dataset updates (push newly exported runs/sims).
- Use Drogon’s built-in WebSocket controller.

---

## 6. Implementation Details

### 6.1 Dataset Export (POST `/api/datasets`)
- Request payload includes slug, OHLCV CSV (multipart upload or remote URL), indicator CSV, metadata (symbol, granularity).
- Server flow:
  1. Validate slug/measurements.
  2. Write OHLCV CSV to temp file → convert to ChronosFlow/Arrow → push to QuestDB via existing `QuestDbDataFrameGateway` logic.
  3. Repeat for indicator frame.
  4. Build row counts/timestamp bounds, call `upsert_stage1_dataset`, upsert `indicator_datasets`.
  5. Return dataset_id + measurement names.

### 6.2 Dataset Load (GET `/api/datasets/{id}`)
- Query `stage1_datasets` for metadata.
- Provide signed URLs (or proxied endpoints) to stream OHLCV/indicator data from QuestDB via `/api/datasets/{id}/ohlcv` etc.

### 6.3 Run persistence (POST `/api/runs`)
- Payload: dataset_id, slug, predictions measurement (optional), serialized config, folds list, metrics JSON, optional clipboard text.
- Server:
  1. Validate dataset exists (FK ensures).
  2. Insert `walkforward_runs` + folds in transaction.
  3. If CSV for predictions provided, send to QuestDB under deterministic measurement name.
  4. Return run_id + measurement.

### 6.4 Simulation persistence (POST `/api/simulations`)
- Similar flow: validate run/dataset, write trades CSV to QuestDB, insert metadata + trade buckets/trades into Postgres.

### 6.5 QuestDB Streaming
- Build `QuestDbService` with methods:
  - `ExportFrame(measurement, ArrowTable, tags)`
  - `StreamCsv(sql, HttpResponsePtr resp)` – uses Drogon’s async stream APIs to forward QuestDB `/exp` output directly to clients without storing the whole result.

### 6.6 Error Handling
- Standard JSON error envelope: `{ "error": "message", "details": ... }`
- Map Postgres constraint violations to 409/400 codes.
- For QuestDB errors, relay the exact message retrieved from `/exp`.

---

## 7. Deployment & Ops

1. **Build**: `cmake -B build && cmake --build build --config Release`
2. **Service**: create `stage1_drogon.service` (systemd) to run `/opt/stage1_server/stage1_server --config config.json`.
3. **TLS**: point Drogon to Let’s Encrypt cert; auto-reload on cert renewal.
4. **Logging**: enable Drogon’s async logging to `/var/log/stage1_drogon.log`.
5. **Monitoring**: add health endpoint `/healthz` returning DB connectivity status.

---

## 8. Migration Steps

1. Finish Drogon scaffold (`drogon_ctl create project stage1_server`), generate models.
2. Implement DatasetController + DatasetService (export/import endpoints).
3. Port Stage1MetadataWriter logic into RunController/SimulationController using Postgres transactions.
4. Update desktop app to hit Drogon endpoints instead of direct DB calls (phase-by-phase: dataset export first, then run save/load, then simulation).
5. Once all traffic flows through Drogon, revoke direct Postgres access from desktops (firewall).
6. Extend API for frontend (filters, search, websockets).

---

## 9. Open Questions / TODOs

- **Auth/ACL**: Need a lightweight token management strategy before exposing the API publicly.
- **File uploads**: Max payload size for multipart OHLCV uploads? Possibly support gzip to reduce bandwidth.
- **Backfill**: Provide admin endpoints to re-ingest existing measurements (pull from QuestDB → re-register dataset).
- **Data retention**: Decide whether QuestDB tables are immutable; consider naming scheme `<slug>_ohlcv`/`<slug>_ind`, runs as `<slug>_wf_<timestamp>`, sims `<slug>_sim_<timestamp>`.
- **Web UI**: Plan GraphQL vs REST? (Current plan is REST; can add GraphQL later if needed.)

---

By fronting Postgres/QuestDB with this Drogon API, we isolate all schema quirks, password rotations, and naming rules on the server, while giving both desktop and web clients a stable contract for managing datasets, runs, and simulations. This plan leverages the infrastructure we already validated (Stage 1 Postgres, QuestDB exports) and sets us up for richer UI tooling without exposing raw database credentials again.
